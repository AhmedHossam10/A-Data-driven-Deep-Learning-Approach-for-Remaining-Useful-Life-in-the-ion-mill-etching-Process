{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8339822,"sourceType":"datasetVersion","datasetId":4953259}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import random\n# random.seed(1234)\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport tensorflow as tf\nimport math\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, LSTM, GRU, Bidirectional, Dense, Dropout, Layer, Concatenate, Masking, Attention, Flatten, MultiHeadAttention, BatchNormalization, RepeatVector, Reshape, Conv1D, Add\nfrom tensorflow.keras.regularizers import L1L2, L1, L2\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nimport keras.callbacks\n\nfrom tensorflow.keras import layers, Model\n\nfrom keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 1\n\ntf.config.experimental.enable_op_determinism()\ntf.keras.utils.set_random_seed(random_state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Error(y_pred, y_real):\n    y_pred = np.nan_to_num(y_pred, copy = True)\n    y_real = np.nan_to_num(y_real, copy = True)\n    temp = np.exp(-0.001 * y_real) * np.abs(y_real - y_pred)\n    error = np.sum(temp)\n    return error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def customLoss(y_pred, y_real):\n    return tf.reduce_sum(tf.exp(-0.001 * y_real) * tf.abs(y_real - y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in Data\nsensor_data = pd.read_csv('../input/phm-2018/phm_data_challenge_2018/train/01_M02_DC_train.csv')\nfaults_data = pd.read_csv('../input/phm-2018/phm_data_challenge_2018/train/train_faults/01_M02_train_fault_data.csv')\nttf_data = pd.read_csv('../input/phm-2018/phm_data_challenge_2018/train/train_ttf/01_M02_DC_train.csv')\n\n# sensor_data = sensor_data.drop(['Tool'], axis = 1)\n# sensor_data = sensor_data.drop(['Lot'], axis = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make sure the TTFs are all divisible by 4, and subtract mod 4 if not\nttf_data['TTF_FlowCool Pressure Dropped Below Limit'] = ttf_data['TTF_FlowCool Pressure Dropped Below Limit'].sub(ttf_data['TTF_FlowCool Pressure Dropped Below Limit'] % 4)\nttf_data['TTF_Flowcool Pressure Too High Check Flowcool Pump'] = ttf_data['TTF_Flowcool Pressure Too High Check Flowcool Pump'].sub(ttf_data['TTF_Flowcool Pressure Too High Check Flowcool Pump'] % 4)\nttf_data['TTF_Flowcool leak'] = ttf_data['TTF_Flowcool leak'].sub(ttf_data['TTF_Flowcool leak'] % 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set failure time to mod 4 as well so the time index matches other datasets\nfaults_data['time'] = faults_data['time'].sub(faults_data['time'] % 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#join the ttf and training data together on time\ndf = pd.concat([sensor_data, ttf_data], axis=1, join = 'inner')\ndf.columns = ['time', 'Tool', 'stage', 'Lot', 'runnum', 'recipe', 'recipe_step','IONGAUGEPRESSURE', 'ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT','ETCHSUPPRESSORVOLTAGE', 'ETCHSUPPRESSORCURRENT', 'FLOWCOOLFLOWRATE','FLOWCOOLPRESSURE', 'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK','FIXTURETILTANGLE', 'ROTATIONSPEED', 'ACTUALROTATIONANGLE','FIXTURESHUTTERPOSITION', 'ETCHSOURCEUSAGE', 'ETCHAUXSOURCETIMER','ETCHAUX2SOURCETIMER', 'ACTUALSTEPDURATION', 'time_drop','TTF_FlowCool Pressure Dropped Below Limit','TTF_Flowcool Pressure Too High Check Flowcool Pump','TTF_Flowcool leak']\n#drop excess \"time\" column by position\ndf = df.drop(df.columns[24],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FSP of 1 is the only important data\n#df = df.loc[df['FIXTURESHUTTERPOSITION'] == 1]\n#drop NaNs that are present at the end of ttf (no more failures)\ndf = df.fillna(method = 'ffill')\n# (subset=['TTF_FlowCool Pressure Dropped Below Limit','TTF_Flowcool Pressure Too High Check Flowcool Pump', 'TTF_Flowcool leak'], how='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop duplicates ignoring the time and TTF columns\ndf.drop_duplicates(subset=df.columns.difference(['time', 'TTF_FlowCool Pressure Dropped Below Limit', 'TTF_Flowcool Pressure Too High Check Flowcool Pump',\t'TTF_Flowcool leak']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make 3 dataframes, one for each fail type\ndf_f1 = df.drop(['TTF_Flowcool Pressure Too High Check Flowcool Pump', 'TTF_Flowcool leak', \"time\", \"Tool\", \"ROTATIONSPEED\"],axis = 1)\ndf_f1 = df_f1.dropna()\ndf_f2 = df.drop(['TTF_FlowCool Pressure Dropped Below Limit', 'TTF_Flowcool leak', \"time\", \"Tool\", \"ROTATIONSPEED\"],axis = 1)\ndf_f2 = df_f2.dropna()\ndf_f3 = df.drop(['TTF_FlowCool Pressure Dropped Below Limit', 'TTF_Flowcool Pressure Too High Check Flowcool Pump', \"time\", \"Tool\", \"ROTATIONSPEED\"],axis = 1)\ndf_f3 = df_f3.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"stage, recipe, and recipe step are categorical and need to be encoded as such, but adds complexity. Wu dropped them. include OHE in Proprocess_data function","metadata":{}},{"cell_type":"code","source":"df_f3['TTF_Flowcool leak'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TTF_Flowcool leak'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prod\ndef PreProcess_Data(df_f1, df_f2, df_f3, numKept, numFail):\n    #only keep the data within 6000 seconds of a failure  to closer analyze the data\n    df_f1 = df_f1.loc[df['TTF_FlowCool Pressure Dropped Below Limit'] < numKept]\n    df_f2 = df_f2.loc[df['TTF_Flowcool Pressure Too High Check Flowcool Pump'] < numKept]\n    df_f3 = df_f3.loc[df['TTF_Flowcool leak'] < numKept]\n\n    # df_f1['IsFailure'] = np.where(df_f1['TTF_FlowCool Pressure Dropped Below Limit'] < numFail, True, False)\n    # df_f2['IsFailure'] = np.where(df_f2['TTF_Flowcool Pressure Too High Check Flowcool Pump'] < numFail, True, False)\n    # df_f3['IsFailure'] = np.where(df_f3['TTF_Flowcool leak'] < numFail, True, False)\n    df_f1.loc[df_f1['TTF_FlowCool Pressure Dropped Below Limit'] < numFail, 'IsFailure'] = 1\n    df_f1.loc[df_f1['TTF_FlowCool Pressure Dropped Below Limit'] >= numFail, 'IsFailure'] = 0\n\n    df_f2.loc[df_f2['TTF_Flowcool Pressure Too High Check Flowcool Pump'] < numFail, 'IsFailure'] = 1\n    df_f2.loc[df_f2['TTF_Flowcool Pressure Too High Check Flowcool Pump'] >= numFail, 'IsFailure'] = 0\n\n    df_f3.loc[df_f3['TTF_Flowcool leak'] < numFail, 'IsFailure'] = 1\n    df_f3.loc[df_f3['TTF_Flowcool leak'] >= numFail, 'IsFailure'] = 0\n\n    return df_f1, df_f2, df_f3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1, df2, df3 = PreProcess_Data(df_f1, df_f2, df_f3, 10800 , 1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['TTF_Flowcool leak'].min()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n#get data with 5000 points from 0 as relevant and 250 point from 0 being fail data\n#df1, df2, df3 = PreProcess_Data(df_f1, df_f2, df_f3, 86400, 20000)\n\n#drop categorical columns\ndf1_temp = df1[['TTF_FlowCool Pressure Dropped Below Limit', 'IsFailure']]\ndf2_temp = df2[['TTF_Flowcool Pressure Too High Check Flowcool Pump', 'IsFailure']]\ndf3_temp = df3[['TTF_Flowcool leak', 'IsFailure']]\ndf1 = df1.drop(['stage', 'recipe', 'recipe_step', 'Lot', 'runnum', 'TTF_FlowCool Pressure Dropped Below Limit', 'IsFailure'], axis=1)\ndf2 = df2.drop(['stage', 'recipe', 'recipe_step', 'Lot', 'runnum', 'TTF_Flowcool Pressure Too High Check Flowcool Pump', 'IsFailure'], axis=1)\ndf3 = df3.drop(['stage', 'recipe', 'recipe_step', 'Lot', 'runnum', 'TTF_Flowcool leak', 'IsFailure'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#implement a standard scaler to normalize the dataset, but not scaling the target values.\nscaler = preprocessing.MinMaxScaler()\n# scaler.fit(df1)\n# df1_scaled = scaler.transform(df1)\ndf1_scaled = scaler.fit_transform(df1)\ndf2_scaled = scaler.fit_transform(df2)\ndf3_scaled = scaler.fit_transform(df3)\ndf1_scaled = pd.DataFrame(df1_scaled, columns = df1.columns)\ndf2_scaled = pd.DataFrame(df2_scaled, columns = df2.columns)\ndf3_scaled = pd.DataFrame(df3_scaled, columns = df3.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reintroduce the target values\ndf1_index = pd.DataFrame(df1_temp.index.values)\ndf2_index = pd.DataFrame(df2_temp.index.values)\ndf3_index = pd.DataFrame(df3_temp.index.values)\n\ndf1_scaled = pd.concat([df1_scaled, df1_index], axis=1, join = 'inner')\ndf1_scaled = df1_scaled.set_index(0)\ndf1_scaled = pd.concat([df1_scaled, df1_temp ], axis=1, join = 'inner')\n\ndf2_scaled = pd.concat([df2_scaled, df2_index], axis=1, join = 'inner')\ndf2_scaled = df2_scaled.set_index(0)\ndf2_scaled = pd.concat([df2_scaled, df2_temp ], axis=1, join = 'inner')\n\ndf3_scaled = pd.concat([df3_scaled, df3_index], axis=1, join = 'inner')\ndf3_scaled = df3_scaled.set_index(0)\ndf3_scaled = pd.concat([df3_scaled, df3_temp ], axis=1, join = 'inner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_eda = df1_scaled.reset_index()\ndf1_eda_vis = df1_eda.drop(['index', 'TTF_FlowCool Pressure Dropped Below Limit'], axis=1)\ndf1_eda_tar = df1_eda['TTF_FlowCool Pressure Dropped Below Limit']\n\ndf2_eda = df2_scaled.reset_index()\ndf2_eda_vis = df2_eda.drop(['index', 'TTF_Flowcool Pressure Too High Check Flowcool Pump'], axis=1)\ndf2_eda_tar = df2_eda['TTF_Flowcool Pressure Too High Check Flowcool Pump']\n\ndf3_eda = df3_scaled.reset_index()\ndf3_eda_vis = df3_eda.drop(['index', 'TTF_Flowcool leak'], axis=1)\ndf3_eda_tar = df3_eda['TTF_Flowcool leak']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n#columns =df1_eda_vis.columns.drop('IsFailure')\ncolumns = ['IONGAUGEPRESSURE', 'ETCHBEAMVOLTAGE', 'FLOWCOOLFLOWRATE',\t'FLOWCOOLPRESSURE', 'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK', \n           'FIXTURETILTANGLE', 'ACTUALROTATIONANGLE', 'ETCHSOURCEUSAGE', 'ACTUALSTEPDURATION']\ngrid = gridspec.GridSpec(7, 3)\n\nplt.figure(figsize=(20,30))\n\nfor n, col in enumerate(df1_eda_vis[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df1_eda_vis[df1_eda_vis.IsFailure==0][col], bins = 50, color='g', label = 'Normal') \n    sns.distplot(df1_eda_vis[df1_eda_vis.IsFailure==1][col], bins = 50, color='r', label = 'Faliure')\n    \n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\n    ax.legend()\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns =df2_eda_vis.columns.drop('IsFailure')\ngrid = gridspec.GridSpec(7, 3)\n\nplt.figure(figsize=(20,30))\n\nfor n, col in enumerate(df2_eda_vis[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df2_eda_vis[df2_eda_vis.IsFailure==0][col], bins = 50, color='g', label = 'Normal') \n    sns.distplot(df2_eda_vis[df2_eda_vis.IsFailure==1][col], bins = 50, color='r', label = 'Faliure')\n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\n    ax.legend()\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns =df3_eda_vis.columns.drop('IsFailure')\n\ngrid = gridspec.GridSpec(7, 3)\n\nplt.figure(figsize=(20,30))\n\nfor n, col in enumerate(df3_eda_vis[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df3_eda_vis[df3_eda_vis.IsFailure==0][col], bins = 50, color='g', label = 'Normal') \n    sns.distplot(df3_eda_vis[df3_eda_vis.IsFailure==1][col], bins = 50, color='r', label = 'Faliure')\n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\n    ax.legend()\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols1 = df1_eda_vis.columns.drop('IsFailure')\n# for column in cols1:\n#     plt.figure(figsize = (30, 3))\n#     plt.plot(df1_eda.index, df1_eda_vis[column])\n#     plt.title(column)\n#     for ele in np.where(df1_eda_tar == 0)[0]:\n#         plt.axvline(x = ele, color = 'red')\n\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols2 = df2_eda_vis.columns.drop('IsFailure')\n# for column in cols2:\n#     plt.figure(figsize = (30, 3))\n#     plt.plot(df2_eda.index, df2_eda_vis[column])\n#     plt.title(column)\n#     for ele in np.where(df2_eda_tar == 0)[0]:\n#         plt.axvline(x = ele, color = 'red')\n\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols3 = df3_eda_vis.columns.drop('IsFailure')\n# for column in cols3:\n#     plt.figure(figsize = (30, 3))\n#     plt.plot(df3_eda_vis.index, df3_eda_vis[column])\n#     plt.title(column)\n#     for ele in np.where(df3_eda_tar == 0)[0]:\n#         plt.axvline(x = ele, color = 'red')\n\n#     plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_eda_vis.describe().apply(lambda s: s.apply('{0:.5f}'.format)).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2_eda_vis.describe().apply(lambda s: s.apply('{0:.5f}'.format)).T\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3_eda_vis.describe().apply(lambda s: s.apply('{0:.5f}'.format)).T\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set()\n\nplt.figure(figsize=(14,8))\nsns.heatmap(df1_eda_vis.corr(), annot=True, cmap ='crest', fmt='.2f',linewidths=.5)\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.heatmap(df2_eda_vis.corr(), annot=True, cmap ='crest', fmt='.2f',linewidths=.5)\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nsns.heatmap(df3_eda_vis.corr(), annot=True, cmap ='crest', fmt='.2f',linewidths=.5)\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sensor_data.index = range(0,len(sensor_data))\n# ttf_data.index = range(0,len(ttf_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sensor_data = sensor_data.drop(['Tool'], axis = 1)\nsensor_data = sensor_data.drop(['Lot'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cutoff(sensor_data, faults_data, ttf_data, column):\n    # cut off the tail of the data set that with NaN ttf\n    temp = faults_data[faults_data['fault_name'] == column]\n    last_failure = temp['time'].values[-1]\n    array = np.asarray(sensor_data['time'])\n    closest_ind = (np.abs(array - last_failure)).argmin()\n    if ((array[closest_ind] - last_failure) != np.abs(array[closest_ind] - last_failure)):\n        ind = closest_ind + 1\n    elif ((array[closest_ind] - last_failure) == 0):\n        ind = closest_ind + 1\n    else:\n        ind = closest_ind\n    sensor_data = sensor_data[:ind]\n    ttf_data = ttf_data[:ind]\n    faults_data = faults_data[faults_data['fault_name'] == column]\n    return sensor_data, ttf_data, faults_data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sensor_fault1, ttf_fault1, faults_fault1 = cutoff(sensor_data, faults_data, ttf_data, 'FlowCool Pressure Dropped Below Limit')    \n\nsensor_fault1 = sensor_fault1.fillna(method = 'ffill')\nsensor_fault1['recipe'] = sensor_fault1['recipe'] + 200\nlabel = ttf_fault1['TTF_FlowCool Pressure Dropped Below Limit']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capture the trends\ntemp = ttf_fault1.shift(1)\ndiff = ttf_fault1['TTF_FlowCool Pressure Dropped Below Limit'] - temp['TTF_FlowCool Pressure Dropped Below Limit']\nidx = diff[diff > 0].index\ntrend_start_time = idx.values\ntrend_start_time = np.insert(trend_start_time, 0, 0)   \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select data points\ndef Select(df, y, start_time, num):\n    col = []\n    y_result = []\n    for t in range(1, len(start_time)):\n        if start_time[t] - start_time[t-1] > num:\n            col.append(df[start_time[t] - num: start_time[t]])\n            y_result.extend(y[start_time[t] - num: start_time[t]])\n        else:\n            col.append(df[start_time[t-1]: start_time[t]])\n            y_result.extend(y[start_time[t-1]: start_time[t]])\n    df_result = pd.concat(col, axis=0)\n    y_result = pd.Series(y_result)\n    return df_result, y_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_select, y_select = Select(sensor_fault1, label, trend_start_time, 2000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shift dataset\ndef series_to_supervised(data, y, n_in=50, dropnan=True):\n    data_col = []\n    y_col = []\n    for i in range (0, n_in):\n        data_col.append(data.shift(i))\n        y_col.append(y.shift(i))\n    result = pd.concat(data_col, axis = 1)\n    label = pd.concat(y_col, axis = 1)\n    if dropnan:\n        result = result[n_in:]\n        label = label[n_in:]\n    return result, label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_select.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_select.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, y = series_to_supervised(df_select, y_select, 10, True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\ny_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))\n\n\nfeature = df_scaler.fit_transform(df)\nlabel = y_scaler.fit_transform(y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train, y_valid, y_test = label[0:16000], label[16000:], label\nX_train, X_valid, x_test = feature[0:16000], feature[16000:], feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape((X_train.shape[0], 10, 22))\nX_valid = X_valid.reshape((X_valid.shape[0], 10, 22))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CausalConv1D(layers.Layer):\n    def __init__(self, filters, kernel_size, dilation_rate):\n        super(CausalConv1D, self).__init__()\n        self.conv = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)\n    \n    def call(self, x):\n        return self.conv(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResidualBlock(layers.Layer):\n    def __init__(self, filters, kernel_size, dilation_rates, dropout_rate):\n        super(ResidualBlock, self).__init__()\n        self.layers = []\n        for dilation_rate in dilation_rates:\n            self.layers.append(CausalConv1D(filters, kernel_size, dilation_rate))\n            self.layers.append(layers.ReLU())\n            self.layers.append(layers.Dropout(dropout_rate))\n        self.conv1x1 = layers.Conv1D(filters, 1) if filters is not None else None\n    \n    def call(self, x):\n        residual = x\n        for layer in self.layers:\n            x = layer(x)\n        if self.conv1x1 is not None:\n            residual = self.conv1x1(residual)\n        return layers.ReLU()(x + residual)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TCNBlock(layers.Layer):\n    def __init__(self, num_channels, kernel_sizes, dilation_rates, dropout_rate):\n        super(TCNBlock, self).__init__()\n        self.residual_blocks = []\n        for filters, kernel_size, dilation_rate in zip(num_channels, kernel_sizes, dilation_rates):\n            self.residual_blocks.append(ResidualBlock(filters, kernel_size, dilation_rate, dropout_rate))\n    \n    def call(self, x):\n        for block in self.residual_blocks:\n            x = block(x)\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass SelfAttention(layers.Layer):\n    def __init__(self, units):\n        super(SelfAttention, self).__init__()\n        self.units = units\n    \n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n        self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n        self.W_o = self.add_weight(shape=(self.units, input_shape[-1]), initializer='random_normal', trainable=True)\n    \n    def call(self, inputs):\n        q = tf.tensordot(inputs, self.W_q, axes=1)\n        k = tf.tensordot(inputs, self.W_k, axes=1)\n        v = tf.tensordot(inputs, self.W_v, axes=1)\n        \n        score = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.units, tf.float32))\n        attention_weights = tf.nn.softmax(score, axis=-1)\n        \n        context = tf.matmul(attention_weights, v)\n        output = tf.tensordot(context, self.W_o, axes=1)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input layer\ninput_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n\n# TCN Path\np1 = TCNBlock(\n    num_channels=[32, 16],\n    kernel_sizes=[4, 8],\n    dilation_rates=[[1, 2, 4], [1, 2]],\n    dropout_rate=0.4\n)(input_layer)\n\n# LSTM Path\np2 = layers.LSTM(128, activation='tanh', return_sequences=True)(input_layer)\np2 = layers.LSTM(128, activation='tanh', return_sequences=True)(p2)\n\n# Concatenation stage\nx = layers.Concatenate()([p1, p2])\n\n# Additional LSTM layer\nx = layers.LSTM(64, activation='tanh', return_sequences=True)(x)\n\n# Self-Attention Mechanism\nx = SelfAttention(32)(x)\n\n# Flatten\nx = layers.Flatten()(x)\n\n# Prediction Block\nx = layers.Dense(16, activation='relu')(x)\nx = layers.Dense(8, activation='relu')(x)\n\n# output layer\noutput_layer = layers.Dense(1)(x)\n\n# Build the model\nmodel = Model(inputs=input_layer, outputs=output_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(learning_rate=0.001)\nmodel.compile(loss='mean_squared_error', metrics=[\"mse\"], optimizer=opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model,\n           show_shapes=True, \n           show_layer_names=True, \n           expand_nested=True, \n           dpi=50,\n           to_file='test.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callback to early stopping of Training\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    mode=\"min\",\n#     min_delta=0.000005,\n    patience=30,\n    verbose=1,\n    restore_best_weights=True,\n)\n\n\nmodel_callbacks = [\n    early_stopping,\n]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=1000, batch_size=256, validation_data=(X_valid, y_valid), verbose=2, shuffle=False, callbacks= model_callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
